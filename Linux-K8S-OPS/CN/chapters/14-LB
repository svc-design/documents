

ALB / NLB 如何动态加载后端 Upstream

在 AWS 中，ALB 和 NLB 都不会直接管理 upstream 列表；它们通过 Target Group 动态管理后端服务器。

✔ 1. 使用 Target Group 动态注册后端

后端实例（EC2、IP、Lambda）可以通过以下方式动态加入或移除：

（1）自动注册：Auto Scaling Group（ASG）

ASG 可以绑定一个或多个 Target Group

随着实例扩缩容，实例会自动注册/注销至 Target Group

ALB/NLB 自动感知，无需重启

（2）手动注册：AWS Console / CLI / SDK

使用 AWS CLI 注册：

aws elbv2 register-targets \
  --target-group-arn TG_ARN \
  --targets Id=i-xxxxxx

（3）基于 IP 的 Target Group（IP mode）

常用于：

容器平台（ECS、Kubernetes）

On-prem / Hybrid 环境

服务 Mesh

后端容器的 IP 自动注册至 Target Group，ALB/NLB 即可动态负载。

（4）ECS + ALB/NLB (Service Connect)

ECS 服务可自动将任务 IP 注册到 Target Group，实现自动滚动更新。

（5）Kubernetes + AWS Load Balancer Controller

K8s Service 类型为 LoadBalancer + Annotation 指定 ALB/NLB 时：

Pod 的 ENI/IP 会自动注册到 Target Group

滚动更新时自动维护 upstream

🟦 二、ALB 支持的负载均衡方式（L7）

ALB 是 应用层（HTTP/HTTPS）负载均衡器，支持以下均衡算法：

1. Round Robin（轮询）

默认算法。

2. Least Outstanding Requests（最少未完成请求）

适用于后端性能差异大的情况下。

3. Weighted Target Groups （加权 Target Group）

使用 Listener Rule 可给多个 Target Group 设置 Weight，实现：

灰度发布
蓝绿部署

4. Path / Host Routing（基于路径或域名的流量分发）

类似 L7 路由：
/api/* → TG A
/static/* → TG B
user.x.com → TG U

5. Header-based Routing（基于 Header）

用于 A/B Test：
header: X-Version: v2 → TG2

6. HTTP Method Routing（GET/POST 分流）
🟩 三、NLB 支持的负载均衡方式（L4）

NLB 是 传输层（TCP/UDP、TLS）负载均衡器，支持的算法较少：

1. Flow Hash（默认算法）
通过以下字段计算 hash，将同一连接保持到同一后端（会话一致性）：
源/目的 IP
源/目的 port

协议

2. Weighted Target Groups（权重）
与 ALB 类似，也支持多个 Target Group 的加权流量分发（NLB 2019+）。

适用于：

蓝绿部署
灰度发布

3. Preserve Client IP（保持客户端 IP）

但这是功能，不是算法，常与 L4 负载配套。

🔵 四、ALB vs NLB 的动态 upstream 特性对比
功能	ALB	NLB
动态注册 upstream	✔ Target Group	✔ Target Group
自动从 ASG/ECS/K8s 注册	✔	✔
L7 路由能力	强	无
负载均衡算法	轮询/最少请求/权重	Flow Hash/权重
灰度/蓝绿	✔（加权 TG）	✔（加权 TG）
健康检查	L7（HTTP）	L4（TCP/HTTP）
🔶 五、总结
ALB/NLB 动态加载 upstream 的核心机制：Target Group 自动注册后端

来源可以是：

Auto Scaling Group
ECS/EKS（容器）
手动注册
IP-based（跨网络环境）

负载均衡方式

ALB（L7）：Round Robin / Least Requests / Weighted / Path/Host routing / Header routing
NLB（L4）：Flow Hash / Weighted

如果你需要，我可以进一步提供：
ALB/NLB 配置示例（Terraform/CloudFormation）
ASG + ALB 自动注册示例

Kubernetes 通过 AWS Load Balancer Controller 的完整 YAML 示例


nginx 的语境下，AWS 的 Target Group（TG） 最接近下面两个概念：

✅ Target Group 相当于 nginx 的 upstream 块（upstream pool）

在 nginx 中：

upstream backend {
    server 10.1.1.1;
    server 10.1.1.2;
}


这个 upstream backend 就是一组后端服务器列表。

👉 AWS Target Group 的作用与 nginx 的 upstream 基本一致：

它代表一组可被负载均衡到的后端目标（EC2、IP、容器、Lambda）

包含健康检查

包含负载算法

配置在 LB（ALB/NLB）的 listener rule 中被引用

🔍 更详细的对应图
AWS 架构	nginx 架构
ALB / NLB（负载均衡器）	server { listen ... } 的 frontend
Target Group	upstream {} 后端池
Target	upstream 内的 server
Health Check	nginx health_check（或 via nginx plus）
Listener Rule → TG	location /api { proxy_pass http://backend; }
🔧 为什么说它更像 upstream？

因为：

Target Group 是后端服务器集合

一个 LB 可以指向多个 Target Group（类似多个 upstream pool）

TG 自己不处理请求，只定义后端目标

LB 根据规则将流量转发给某个 TG

这与 nginx 的概念完全一致。

🔎 对比示例
nginx
upstream api_backend {
    server 10.0.1.10;
    server 10.0.1.11;
}

server {
    listen 80;

    location /api/ {
        proxy_pass http://api_backend;
    }
}

AWS ALB
Listener (port 80)
 └── Rule: path /api/* → Target Group: api-backend


Target Group:

Targets:
- 10.0.1.10
- 10.0.1.11
Health Check: /health

🔵 补充：Target Group 比 upstream 更强的地方
功能	nginx upstream	AWS Target Group
健康检查	需要 plus 或自写脚本	内置
自动注册（ASG/ECS/K8s）	无	有
跨网络（IP mode）	有限制	原生
Lambda 支持	无	有
多 TG 加权流量	需自写	原生
🟩 总结一句话

AWS Target Group 就是 nginx 的 upstream 服务器池（upstream block）。
Listener Rule 决定“哪个 location → 哪个 upstream（TG）”。

如果你愿意，我可以继续帮你生成：

nginx upstream ↔︎ Target Group 的完整映射表
Terraform / CDK 创建 TG 的示例
Nginx → ALB 迁移推荐架构


下面深入解释 Nginx 自身（开源版）内部是如何实现健康节点维护的，包括它本身能做什么、不能做什么、依赖哪些机制，以及与 Nginx Plus 的差异。

🟥 先给最核心的结论
开源版 Nginx = 没有主动健康检查（Active Health Check）！

开源 Nginx 在 upstream 后端健康管理上：

✔ 有 被动健康检查（Passive Health Check）

✖ 没有 主动健康检查（Active Health Check）

✖ upstream 列表不能自动动态增减（需要 reload/信号/外部工具）

而 Nginx Plus 有完整主动健康检查和动态 upstream API。

🟩 一、开源 Nginx 如何保证健康节点？

开源版主要用 被动方式（passive health check） 来剔除“坏节点”。

✔ 1. 被动健康检查：通过失败的请求判断节点是否宕掉

内部逻辑：

Nginx 把请求转发给 upstream（后端）

如果出现以下情况：

upstream 超时

upstream 返回 500/502/503/504

连接失败

upstream 读写失败

Nginx 认为这个 upstream “可能不健康”

如果连续失败次数达到设定阈值
→ Nginx 会将它标记为“不可用”（marked as failed）

一段时间后（fail_timeout）Nginx 再尝试恢复它

配置示例：
upstream backend {
    server 10.0.0.1 max_fails=3 fail_timeout=10s;
    server 10.0.0.2 max_fails=3 fail_timeout=10s;
}


内部行为：

连续 3 次 request fail → 标记下线

在 10 秒内不再向这个 server 发送流量

10 秒后重试一次，如果成功就恢复

⭐ 特点

被动机制：只有在真实用户请求失败后才认定不健康

不精确：可能把短暂网络抖动误判为 unhealthy

恢复慢：fail_timeout 期间不会发送流量，除非 retry

✔ 2. upstream 轮询机制中跳过 unhealthy 节点

当某个 upstream 被标记为 failed，Nginx 的负载均衡逻辑会：

不再选它作为选取目标

在下一轮周期才重新考虑它（fail_timeout 结束后）

因此从外部看，似乎是“自动剔除后端”。

但本质上是：

被动跳过，而不是健康检查机制主动发现。

✔ 3. 上游再恢复时，需要一个 real request 来测试

Nginx 不会主动探测
它会在 fail_timeout 到期后
用 real traffic 重试一次
若成功 → 恢复

🟧 二、开源 nginx 做不到什么？

开源 nginx 做不到：

❌ 主动探测健康（如 HTTP GET /health 每3秒）
❌ 定时检查后端是否恢复
❌ 不健康节点自愈逻辑
❌ 动态添加/删除 upstream 节点（无原生 API）
❌ 无状态共享（worker 之间没有共享健康状态）

所有这些都是 Nginx Plus 才能做的。

🟦 三、所以行业是如何增强开源 Nginx 的？
✔ 1. 使用 consul-template / etcd / zookeeper + lua 动态更新 upstream

利用：

consul health check

consul-template 渲染 nginx upstream 文件

nginx reload

或

OpenResty + Lua 动态维护


不同发行版（Nginx Open Source / Nginx Plus / OpenResty / Tengine / Kong 的 Nginx 内核）是如何实现动态健康节点的？有什么差异？

下面分发行版讲清楚每一种 健康检查机制的能力与实现方式。

🟩 1. Nginx Open Source（官方开源版）

👉 只有被动健康检查（passive health check）

实现机制：

当请求转发到 upstream 时，如果连接失败/超时/5xx → 增加 failure 计数

达到 max_fails → 标记 server "down"

在 fail_timeout 期间不再选择该 server

到期后使用 真实流量 重试一次，成功则恢复

示例：
upstream backend {
    server 10.0.0.1 max_fails=3 fail_timeout=10s;
}

特点:

✔ 自动剔除坏节点
✖ 没有主动健康检查
✖ 恢复慢
✖ upstream 不能真正动态更新（需 reload）

🟦 2. Nginx Plus（商业版）

👉 唯一官方完全支持“动态 upstream + 主动健康检查”的发行版

内置能力：

✔ 主动健康检查（Active Health Check）

✔ TCP/HTTP/HTTPS 健康探测

✔ 后台定期 ping 后端，不依赖真实流量

✔ 自动剔除不健康节点

✔ 恢复时自动加入

✔ upstream API（无需 reload 动态增删节点）

✔ 共享健康状态（workers 之间同步）

示例：主动健康检查
upstream backend {
    zone backend 64k;
    server 10.0.0.1;
    server 10.0.0.2;
}

server {
    health_check uri=/health interval=5s fails=3 passes=2;
}

特点（企业级）：

👍 真正的动态 upstream
👍 真正的健康检查
👍 可替代 consul/sidecar

🟧 3. OpenResty（Nginx + LuaJIT）

👉 有能力实现“主动健康检查 + 动态 upstream”，但需要 Lua 模块

核心模块：lua-resty-upstream-healthcheck

能力：

✔ 主动健康检查（HTTP/TCP）

✔ 定时探测

✔ 自动标记 unhealthy

✔ 自动恢复

✔ 动态 upsteam（基于 lua_shared_dict）

✔ 无需 reload

示例（OpenResty）：
local hc = require "resty.upstream.healthcheck"

hc.spawn_checker{
    shm = "healthcheck",
    upstream = "backend",
    type = "http",
    http_req = "GET /health HTTP/1.0\r\n\r\n",
    interval = 5000,
    fall = 3,
    rise = 2,
}

特点：

👍 灵活可定制
👍 适合高动态场景（K8s、API 网关）
⚠ 需要 Lua 能力
⚠ 不是官方内置，需要手工集成

🟩 4. Tengine（阿里开源版 Nginx 扩展）

👉 有较明显增强，包括简单的 主动健康检查模块

内置模块：ngx_http_upstream_check_module

能力：

✔ 主动健康检查（HTTP/TCP）

✔ 定时探测

✔ 失败阈值控制

✔ 自动剔除

示例：
upstream backend {
    server 10.0.0.1;
    server 10.0.0.2;

    check interval=3000 rise=2 fall=5 timeout=1000 type=http;
    check_http_send "GET /status HTTP/1.1\r\nHost: localhost\r\n\r\n";
    check_http_expect_alive http_2xx http_3xx;
}

特点：

✔ 不错的主动检查
⚠ upstream 动态更新仍需要 reload（不像 Nginx Plus 可以 API 更新）

🟦 5. Kong / APISIX（基于 OpenResty）

👉 都是自带 Lua 健康检查机制的 API 网关

两者都支持：

✔ 主动健康检查

✔ 被动健康检查

✔ 动态 upstream（无需 reload）

✔ 与注册中心联动（Consul/Etcd）

相当于…
OpenResty 健康检查能力的企业级封装。

🔥 总结对比（最重要）
发行版	被动健康检查	主动健康检查	动态 upstream	是否需 reload	用途
Nginx 开源版	✔	✖	✖	✔	简单 L7 LB
Nginx Plus	✔	✔（一流）	✔（API）	✖	企业生产级
OpenResty	✔	✔（Lua 模块）	✔（Lua）	✖	可编程网关
Tengine	✔	✔（内置）	部分	✔	国内常用
Kong/APISIX	✔	✔（企业级）	✔	✖	完整网关
🧨 如果你想问 “哪种最像 AWS Target Group？”

答案：

➡ Nginx Plus
➡ 或 OpenResty/Kong/APISIX（带 Lua 健康检查）

因为它们支持：

主动健康检查

动态 upstream

自动剔除和恢复

与 AWS ALB/NLB 最接近。

如果你愿意，我可以进一步给你：

🔧 各种发行版的完整配置示例

🧰 “如何在开源 nginx 上用 lua 实现 AWS Target Group 一样的健康检查”

🧱 “如何选适合你业务的 nginx 发行版（网关 vs LB）”

你想深入哪一个？


各种 Nginx 发行版：完整健康检查配置示例

我分别给你：

Nginx OSS（开源版）

Nginx Plus（商业版）

OpenResty（Lua）

Tengine（阿里版）

APISIX / Kong（网关）

每个都有可运行配置示例。

1️⃣ Nginx OSS（官方开源版） → 被动健康检查

没有主动健康检查
只能依赖上游请求失败时自动标记节点为 down。

nginx.conf
upstream backend {
    server 10.0.0.1:8080 max_fails=3 fail_timeout=10s;
    server 10.0.0.2:8080 max_fails=3 fail_timeout=10s;
}

server {
    listen 80;

    location / {
        proxy_pass http://backend;
        proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
    }
}


✔ 自动剔除失败 3 次的服务
✖ 无主动健康检查
✖ 恢复基于 real traffic

2️⃣ Nginx Plus（商业版） → 主动健康检查 + 动态 upstream

Nginx 官方产品，功能最全。

nginx.conf
upstream backend {
    zone backend 64k;
    server 10.0.0.1;
    server 10.0.0.2;
}

server {
    listen 80;
    proxy_pass http://backend;

    health_check uri=/health interval=3s fails=2 passes=2;
}

支持动态添加节点（无需 reload）
curl -X POST \
    -d '{"server": "10.0.0.3"}' \
    http://localhost:8080/api/6/http/upstreams/backend/servers/


✔ 主动健康检查（HTTP/TCP/HTTPS）
✔ 动态 upstream API
✔ 企业级

3️⃣ OpenResty（Nginx + Lua） → 主动健康检查 + 动态 upstream（Lua）

核心模块：lua-resty-upstream-healthcheck

nginx.conf
lua_shared_dict healthcheck 1m;

upstream backend {
    server 10.0.0.1;
    server 10.0.0.2;
}

init_worker_by_lua_block {
    local hc = require "resty.upstream.healthcheck"

    hc.spawn_checker{
        shm = "healthcheck",
        upstream = "backend",
        type = "http",
        http_req = "GET /health HTTP/1.0\r\nHost: localhost\r\n\r\n",
        interval = 3000,
        fall = 3,
        rise = 2,
    }
}


✔ 主动健康检查（完全模拟 AWS TG）
✔ 无需 reload 即可更新 upstream
✔ 适合高动态容器场景

4️⃣ Tengine（阿里 Nginx 发行版） → 内置主动健康检查

模块：ngx_http_upstream_check_module

配置
upstream backend {
    server 10.0.0.1;
    server 10.0.0.2;

    check interval=3000 rise=2 fall=3 timeout=1000 type=http;
    check_http_send "GET /health HTTP/1.1\r\nHost: localhost\r\n\r\n";
    check_http_expect_alive http_2xx http_3xx;
}


✔ 主动健康检查
✖ upstream 动态性不如 OpenResty/Nginx Plus

5️⃣ APISIX / Kong（网关） → 最强生命力 + 注册中心集成
APISIX 示例（最强 Lua 生态）
routes:
  - uri: /api/*
    upstream:
      type: roundrobin
      nodes:
        "10.0.0.1:8080": 1
        "10.0.0.2:8080": 1
      checks:
        active:
          type: http
          http_path: "/health"
          interval: 3
          healthy:
            successes: 2
          unhealthy:
            http_failures: 2


✔ 主动健康检查
✔ 动态 upstream（Admin API）
✔ 支持 Nacos / Consul / Etcd

🧰 二、如何在开源 Nginx 上用 Lua 实现“一模一样的 AWS Target Group”

AWS Target Group 的关键功能：

✔ 主动健康检查

✔ 自动剔除

✔ 自动恢复

✔ 动态添加/删除节点（Target 注册）

✔ 不需要 reload

✔ 多 worker 共享状态

开源 Nginx 本身不支持，但 OpenResty 可以实现 100% 等价行为。

🔥 完整 OpenResty 实现示例（最接近 AWS TG）
1. 定义共享字典
lua_shared_dict backend_nodes 10m;
lua_shared_dict healthcheck 10m;

2. 动态 upstream（用 Lua 接管 upstream）
init_worker_by_lua_block {
    local hc = require "resty.upstream.healthcheck"
    local lrucache = require "resty.lrucache"

    upstream_cache = lrucache.new(100)

    hc.spawn_checker{
        shm = "healthcheck",
        upstream = "backend",
        type = "http",
        http_req = "GET /health HTTP/1.0\r\n\r\n",
        interval = 3000,
        fall = 3,
        rise = 2,
    }
}

3. 在请求阶段动态选择健康节点
balancer_by_lua_block {
    local balancer = require "ngx.balancer"
    local hc = require "resty.upstream.healthcheck"

    local peers = hc.get_healthy_peers("backend")
    local peer = peers[math.random(#peers)]

    balancer.set_current_peer(peer.host, peer.port)
}

4. 动态添加节点（模拟 AWS register-targets）
location /add {
    content_by_lua_block {
        local args = ngx.req.get_uri_args()
        local host = args.host
        local port = args.port

        ngx.shared.backend_nodes:set(host .. ":" .. port, 1)
    }
}

📌 得到功能：
AWS Target Group 功能	OpenResty 实现
主动健康检查	Lua 模块
自动剔除/恢复	healthcheck 模块
动态注册 target	Lua Admin API
无需 reload	Lua 生效
轮询/随机	Lua balancer

几乎百分百等价。

🧱 三、如何选适合你的业务的 Nginx 发行版？（超级清晰指南）

按业务类型分类：

🟦 1. 普通 Web 站点 / 静态内容

使用：

Nginx OSS

理由：

稳定、够用

不需要动态 upstream

不需要主动健康检查

🟩 2. 传统后端服务 / 内网微服务（少动态变化）

使用：

Tengine

或 OpenResty（轻度）

理由：

有主动健康检查

配置简单

兼容开源 Nginx

🟧 3. 高动态微服务 / 容器环境 / Kubernetes / 注册中心

使用：

OpenResty

APISIX

Kong

理由：

动态 upstream

主动健康检查

热更新

配合 Consul / Etcd / Nacos

推荐顺序：
APISIX > Kong > OpenResty（Lua 自写）

🟨 4. 需要企业级 LB/网关（金融、电商核心链路）

使用：

Nginx Plus（如果预算够）

或 APISIX（更强）

理由：

稳定

官方支持

完整健康检查

完整 API 动态管理

🟥 5. 超高流量、超高动态场景（等价 AWS TG）

三选一：

APISIX（最强）

OpenResty + Lua healthcheck

Nginx Plus（付费）

🎯 最终选型一句话总结
场景	最佳选择
简单站点	Nginx OSS
需要健康检查	Tengine
微服务 / 动态 upstream	OpenResty / APISIX
企业级、预算足	Nginx Plus
按 AWS TG 逻辑运行	OpenResty / Nginx Plus
如果你需要，我可以继续输出：

🗂 所有发行版的目录结构、打包方式（Yum/Apt 源）

☸ K8s 里各发行版的部署 YAML

🚀 OpenResty 完整工程模板（可跑健康检查）

💡 如何从 Nginx 迁移到 ALB/NLB 的续航方案
